{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formal\n",
      "1400-01-01    6\n",
      "1407-05-03    6\n",
      "1407-04-21    6\n",
      "1407-04-22    6\n",
      "1407-04-23    6\n",
      "             ..\n",
      "1403-09-03    6\n",
      "1403-09-04    6\n",
      "1403-09-05    6\n",
      "1403-09-06    6\n",
      "1410-12-29    6\n",
      "Name: count, Length: 4015, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"date.csv\")\n",
    "print(data[\"formal\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_portion = int(0.8 * (len(data)))\n",
    "test_portion = int(0.1 * (len(data)))\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion: train_portion + test_portion]\n",
    "val_data = data[train_portion+test_portion :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 19272\n",
      "Validation set length: 2409\n",
      "Test set length: 2409\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>informal</th>\n",
       "      <th>formal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>اول فروردین هزار و چهار صد و</td>\n",
       "      <td>1400-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>روز 1 فروردین هزار و چهار صد و</td>\n",
       "      <td>1400-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 فروردین هزار و چهار صد و</td>\n",
       "      <td>1400-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2 فروردین هزار و چهار صد و</td>\n",
       "      <td>1400-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>روز 2 فروردین هزار و چهار صد و</td>\n",
       "      <td>1400-01-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          informal      formal\n",
       "0    اول فروردین هزار و چهار صد و   1400-01-01\n",
       "1  روز 1 فروردین هزار و چهار صد و   1400-01-01\n",
       "2      1 فروردین هزار و چهار صد و   1400-01-01\n",
       "3      2 فروردین هزار و چهار صد و   1400-01-02\n",
       "4  روز 2 فروردین هزار و چهار صد و   1400-01-02"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class DateData(Dataset):\n",
    "    def __init__(self,data,tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_prob = 0.2\n",
    "        self.pad_token_id = 6\n",
    "        self.mask_token_id = 27\n",
    "        self.data = data\n",
    "        self.encoded_data = []\n",
    "        self.informal = []\n",
    "        self.formal = []\n",
    "        for _,row in data.iterrows():\n",
    "            self.informal.append(row[\"informal\"])\n",
    "            self.formal.append(row[\"formal\"])\n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "            input_ids = torch.tensor(self.tokenizer.encode(\n",
    "                 self.informal[idx] +\" \"+\"[MASK][MASK][MASK][MASK]-[MASK][MASK]-[MASK][MASK]\"))\n",
    "            \n",
    "            labels = torch.tensor(self.tokenizer.encode(self.informal[idx] +\" \"+ self.formal[idx]))\n",
    "            \n",
    "\n",
    "                    \n",
    "            return input_ids, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_data:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\project\\evo\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./persianTokenizer\")\n",
    "pecial_tokens_dict = {'additional_special_tokens': [\"[<year>]\",\"[<month>]\",\"[<day>]\"]}\n",
    "tokenizer.add_special_tokens(pecial_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DateData(train_data,tokenizer)\n",
    "test_dataset = DateData(test_data,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim, dropout=0.1,device=\"cuda\"):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(output_dim, model_dim)\n",
    "        self.positional_encoding = torch.nn.Embedding(input_dim, model_dim)\n",
    "        self.en = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(d_model=model_dim,nhead=num_heads,dim_feedforward= model_dim * 2,\n",
    "                                         activation = torch.nn.functional.gelu,\n",
    "                                         batch_first=True,bias=False,dropout=0.1,device=device),\n",
    "                                           num_layers=num_layers,enable_nested_tensor=False)\n",
    "        self.fc_train = torch.nn.Linear(model_dim, output_dim)\n",
    "        self.fc_year = torch.nn.Linear(model_dim, 10)\n",
    "        self.fc_day = torch.nn.Linear(model_dim, 31)\n",
    "        self.fc_month = torch.nn.Linear(model_dim, 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        tok_embed = self.embedding(x)\n",
    "        pos_embed = self.positional_encoding(torch.arange(seq_len, device=x.device))\n",
    "        x = tok_embed + pos_embed\n",
    "        x = self.en(x)\n",
    "        \n",
    "        return self.fc_train(x)\n",
    "    def forward_year(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        tok_embed = self.embedding(x)\n",
    "        pos_embed = self.positional_encoding(torch.arange(seq_len, device=x.device))\n",
    "        x = tok_embed + pos_embed\n",
    "        x = self.en(x)\n",
    "        \n",
    "        return self.fc_year(x[:,0,:])\n",
    "    \n",
    "    def forward_month(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        tok_embed = self.embedding(x)\n",
    "        pos_embed = self.positional_encoding(torch.arange(seq_len, device=x.device))\n",
    "        x = tok_embed + pos_embed\n",
    "        x = self.en(x)\n",
    "        \n",
    "        return self.fc_month(x[:,1,:])\n",
    "    \n",
    "    def forward_day(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        tok_embed = self.embedding(x)\n",
    "        pos_embed = self.positional_encoding(torch.arange(seq_len, device=x.device))\n",
    "        x = tok_embed + pos_embed\n",
    "        x = self.en(x)\n",
    "        \n",
    "        return self.fc_day(x[:,2,:])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=6,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    \n",
    "    batch_max_length = max(item[0].shape[-1] + 1 for item in batch)\n",
    "    \n",
    "\n",
    "    \n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        \n",
    "        new_item = item[0].numpy().tolist()\n",
    "       \n",
    "        new_item += [pad_token_id]\n",
    "       \n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        new_item1 = item[1].numpy().tolist()\n",
    "       \n",
    "        new_item1 += [pad_token_id]\n",
    "        \n",
    "        padded1 = (\n",
    "            new_item1 + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item1))\n",
    "        )\n",
    "        inputs = torch.tensor(padded)  \n",
    "        targets = torch.tensor(padded1)  \n",
    "\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "      \n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "   \n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=\"cpu\",\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 2048\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,drop_last=True,collate_fn=customized_collate_fn)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False,drop_last=True,collate_fn=customized_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n",
      "torch.Size([512, 25])\n"
     ]
    }
   ],
   "source": [
    "for inputs,targets in train_loader:\n",
    "    print(inputs.shape)\n",
    "    print(targets.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_CONFIG = {\n",
    "    \"vocab_size\" : 25003,\n",
    "    \"context_length\" : 32,\n",
    "    \"emb_dim\" : 256,\n",
    "    \"n_heads\" : 4,\n",
    "    \"n_layers\" : 4,\n",
    "    \"drop_rate\" : 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Embedding(25003, 256)\n",
       "  (positional_encoding): Embedding(32, 256)\n",
       "  (en): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=512, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=256, bias=False)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_train): Linear(in_features=256, out_features=25003, bias=True)\n",
       "  (fc_year): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (fc_day): Linear(in_features=256, out_features=31, bias=True)\n",
       "  (fc_month): Linear(in_features=256, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    input_dim=DATE_CONFIG[\"context_length\"],\n",
    "    model_dim=DATE_CONFIG[\"emb_dim\"],\n",
    "    num_heads=DATE_CONFIG[\"n_heads\"],\n",
    "    num_layers=DATE_CONFIG[\"n_layers\"],\n",
    "    output_dim= DATE_CONFIG[\"vocab_size\"],\n",
    ")\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7598/3531982714.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"bertV2.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Embedding(25003, 256)\n",
       "  (positional_encoding): Embedding(32, 256)\n",
       "  (en): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=512, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=256, bias=False)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_train): Linear(in_features=256, out_features=25003, bias=True)\n",
       "  (fc_year): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (fc_day): Linear(in_features=256, out_features=31, bias=True)\n",
       "  (fc_month): Linear(in_features=256, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"bertV12.pth\"))\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 14,947,552\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch,target_batch,model,device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\project\\evo\\venv\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 10.321034199482686\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader,model,device=\"cuda\")\n",
    "\n",
    "print(\"Train loss =\", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss = 0.010966735891997814\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    test_loss = calc_loss_loader(test_loader,model,device=\"cuda\")\n",
    "\n",
    "print(\"Test loss =\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_test(model,test_loader):\n",
    "    model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = calc_loss_loader(test_loader,model,device=\"cuda\")\n",
    "\n",
    "    print(\"Test loss =\", test_loss)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , total epoch loss 0.005016966878126065\n",
      "Test loss = 0.010966735891997814\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# lr=0.000005\n",
    "epochs = 1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005,weight_decay=0.1)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len(train_loader)*epochs)\n",
    "# optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.\n",
    "    for inputs,targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = calc_loss_batch(inputs,targets,model,\"cuda\")\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    print(f\"Epoch {epoch} , total epoch loss {total_loss / len(train_loader)}\")\n",
    "    calculate_loss_test(model,test_loader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked prompt  [CLS] روز 1 فروردین هزار و چهار صد و [MASK][MASK][MASK][MASK]-[MASK][MASK]-[MASK][MASK][SEP]\n",
      "Model OutPut  [CLS] روز 1 فروردین هزار و چهار صد و 1400-11-01[SEP]\n",
      "Real output [CLS] روز 1 فروردین هزار و چهار صد و 1400-01-01[SEP]\n",
      "Test Masked prompt  [CLS] روز 17 دی 1406 [MASK][MASK][MASK][MASK]-[MASK][MASK]-[MASK][MASK][SEP]\n",
      "Model OutPut  [CLS] روز 17 دی 1406 1406-10-17[SEP]\n",
      "Real output [CLS] روز 17 دی 1406 1406-10-17[SEP]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "masked_prompt , prompt = train_dataset[1]\n",
    "test_masked_prompt , test_prompt = test_dataset[219]\n",
    "masked_prompt = masked_prompt.unsqueeze(0)\n",
    "test_masked_prompt = test_masked_prompt.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    masked_prompt = masked_prompt.to(\"cuda\")\n",
    "    test_masked_prompt = test_masked_prompt.to(\"cuda\")\n",
    "    logits = model(masked_prompt)\n",
    "    logits_test = model(test_masked_prompt)\n",
    "    # logits = logits[:,-1,:]\n",
    "    logits = logits.flatten(0, 1)\n",
    "    logits_test = logits_test.flatten(0, 1)\n",
    "    probs = torch.argmax(logits,dim=-1,keepdim=True)\n",
    "    probs_test = torch.argmax(logits_test,dim=-1,keepdim=True)\n",
    "    token_ids = probs.squeeze(1)\n",
    "    token_ids_test = probs_test.squeeze(1)\n",
    "    \n",
    "\n",
    "print(\"Masked prompt \",token_ids_to_text(masked_prompt,tokenizer))\n",
    "print(\"Model OutPut \",token_ids_to_text(token_ids,tokenizer))\n",
    "print(\"Real output\",token_ids_to_text(prompt,tokenizer))\n",
    "print(\"Test Masked prompt \",token_ids_to_text(test_masked_prompt,tokenizer))\n",
    "print(\"Model OutPut \",token_ids_to_text(token_ids_test,tokenizer))\n",
    "print(\"Real output\",token_ids_to_text(test_prompt,tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_masked(model,tokenizer,input,deivce):\n",
    "    model.eval()\n",
    "    inputs_masked = input + \" \" + \"[MASK][MASK][MASK][MASK]-[MASK][MASK]-[MASK][MASK]\"\n",
    "    input_ids = tokenizer.encode(inputs_masked)\n",
    "    input_ids = torch.tensor(input_ids).to(deivce)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids.unsqueeze(0))\n",
    "        logits = logits.flatten(0, 1)\n",
    "        probs = torch.argmax(logits,dim=-1,keepdim=True)\n",
    "        token_ids = probs.squeeze(1)\n",
    "        answer_ids = token_ids[-11:-1]\n",
    "    return token_ids_to_text(answer_ids,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1402-02-10'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_masked(model,tokenizer,\"۱۲ اردیبهشت 1402\",\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"bertV11.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.6687422166874222\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "T = 0\n",
    "F = 0\n",
    "for masked_prompt,prompt in test_dataset:\n",
    "    masked_prompt = masked_prompt.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        masked_prompt, prompt = masked_prompt.to(\"cuda\"), prompt.to(\"cuda\")\n",
    "        logits = model(masked_prompt)\n",
    "        logits = logits.flatten(0, 1)\n",
    "        probs = torch.argmax(logits,dim=-1,keepdim=True)\n",
    "        token_ids = probs.squeeze(1)\n",
    "        if torch.equal(prompt,token_ids):\n",
    "            T += 1\n",
    "        else:\n",
    "            F += 1\n",
    "        \n",
    "print(\"Accuracy = \", T / (T + F))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on Test data = 1.0110270977020264\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(torch.tensor(test_loss))\n",
    "print(f\"Perplexity on Test data = {perplexity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
